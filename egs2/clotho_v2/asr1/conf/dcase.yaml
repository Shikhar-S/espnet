batch_type: folded
batch_size: 64
accum_grad: 1    # gradient accumulation steps
max_epoch: 100
patience: none
best_model_criterion:  # criterion to save best models
-   - valid
    - acc
    - max
keep_nbest_models: 5  # save nbest models and average these checkpoints
use_amp: true    # whether to use automatic mixed precision
num_att_plot: 0  # do not save attention plots to save time in the demo
num_workers: 2   # number of workers in dataloader


# BEATs implementation takes care of generating mel spectrogram, normalization and specaug
frontend: none
input_size: 1 # important to set input_size to 1 if frontend is none
normalize: none # BEATs code does global mean and variance normalization

encoder: beats
encoder_conf:
    beats_ckpt_path: /home/sbharad2/dcase2024_cp/dcase24task6/pretrained_weights/BEATs_iter3_plus_AS2M_finetuned_on_AS2M_cpt1.pt
    beats_config:
        input_patch_size: 16
        embed_dim: 512
    specaug_config:
        apply_freq_mask: true
        freq_mask_width_range:
        - 0
        - 64
        num_freq_mask: 2
        apply_time_mask: true
        time_mask_width_ratio_range:
        - 0
        - 0.12
        num_time_mask: 5
decoder: hugging_face_transformers
decoder_conf:
    model_name_or_path: facebook/bart-base

# The initilization is only for the linear layer between the BEATs encoder and the BART decoder. 
# BEATs and BART weights override this for encoder and decoder respectively.
init: kaiming_normal

token_type: hugging_face

# Loss, optimizer, scheduler
model_conf:
    ctc_weight: 0.0  # No CTC, only attention branch
    lsm_weight: 0.1  # label smoothing weight
    length_normalized_loss: true

optim: adam
optim_conf:
    lr: 0.00002
    weight_decay: 0.001

scheduler: warmuplr  # linearly increase and exponentially decrease
scheduler_conf:
    warmup_steps: 100