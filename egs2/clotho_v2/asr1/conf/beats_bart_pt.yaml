batch_type: folded
unused_parameters: true
batch_size: 64 # 32
accum_grad: 8 # 2
max_epoch: 80
grad_clip: 2
patience: none
best_model_criterion:  # criterion to save best models
-   - valid
    - acc
    - max
keep_nbest_models: 5  # save nbest models and average these checkpoints
use_amp: true    # whether to use automatic mixed precision
num_att_plot: 0  # do not save attention plots to save time in the demo
num_workers: 2   # number of workers in dataloader


# BEATs implementation takes care of generating mel spectrogram, normalization and specaug
frontend: none
input_size: 1 # important to set input_size to 1 if frontend is none
normalize: none # BEATs code does global mean and variance normalization


freeze_param: [
    "encoder.encoder",
    "encoder.layer_norm",
    "encoder.patch_embedding",
    "encoder.post_extract_proj",
]


encoder: beats
encoder_conf:
    beats_ckpt_path: /compute/babel-13-33/sbharad2/models/BEATs/BEATs_iter3_plus_AS2M_finetuned_on_AS2M_cpt1.pt # /u/sbharadwaj/BEATs_models/BEATs_iter3_plus_AS2M_finetuned_on_AS2M_cpt1.pt # _finetuned_on_AS2M_cpt1
    specaug_config:
        apply_freq_mask: true
        freq_mask_width_range:
        - 0
        - 64
        num_freq_mask: 2
        apply_time_mask: true
        time_mask_width_ratio_range:
        - 0
        - 0.12
        num_time_mask: 5
    adapter_layers: 3
    downsampling_rate: 3 # CNN downsampling over beats encoder
    max_layer: 11 # 0 based index
    use_weighted_representation: true

decoder: hugging_face_transformers
decoder_conf:
    model_name_or_path: facebook/bart-base

# The initilization is only for the linear layer between the BEATs encoder and the BART decoder.
# BEATs and BART weights override this for encoder and decoder respectively.
init: kaiming_uniform

token_type: hugging_face

# Loss, optimizer, scheduler
model_conf:
    ctc_weight: 0.0  # No CTC, only attention branch
    lsm_weight: 0.1  # label smoothing weight
    length_normalized_loss: true  # length normalization
    # BART Dictionary customizations
    ignore_id: 1
    sym_blank: "<pad>"
    sym_sos: "<s>"
    sym_eos: "</s>"
    sym_space: "Ä "

optim: adamw
optim_conf:
    lr: 0.0002 # 2e-4
    weight_decay: 0.001

scheduler: warmuplr
scheduler_conf:
    warmup_steps: 8000 # 2500
