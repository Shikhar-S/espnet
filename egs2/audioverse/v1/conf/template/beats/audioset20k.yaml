unused_parameters: true
init: xavier_normal

batch_type: length

max_epoch: MAX_EPOCH
batch_bins: 20000000 # (16000 hz * 10sec) * 125

accum_grad: 1
grad_clip: 1

optim: adamw
optim_conf:
    lr: LEARNING_RATE
    weight_decay: 1.0e-2
    betas: [0.9, 0.98]

scheduler: CosineAnnealingWarmupRestarts
scheduler_conf:
    first_cycle_steps: 100000
    warmup_steps: WARMUP_STEPS
    max_lr: LEARNING_RATE
    min_lr: 5.0e-6

patience: none
best_model_criterion:
-   - valid
    - epoch_mAP
    - max
keep_nbest_models: 1
use_amp: false  # whether to use automatic mixed precision
num_att_plot: 0
num_workers: 2 # dataloader workers

# BEATs implementation takes care of generating mel spectrogram, normalization and specaug
frontend: none
input_size: 1 # important to set input_size to 1 if frontend is none
normalize: none # BEATs code does global mean and variance normalization

encoder: beats
encoder_conf:
    beats_ckpt_path: CHECKPOINT_PATH
    beats_config:
        layer_wise_gradient_decay_ratio: 0.3
        encoder_layerdrop: 0.1
        dropout: 0.0
    use_weighted_representation: false
    specaug_config:
        apply_time_warp: true
        apply_freq_mask: false
        apply_time_mask: true
        time_mask_width_ratio_range:
        - 0
        - 0.06
        num_time_mask: 1
    roll_augment: true
    roll_interval: 1

model_conf:
    classification_type: multi-label
    mixup_probability: MIXUP_PROB
    mixup_alpha: MIXUP_ALPHA
    lsm_weight: 0.1  # label smoothing weight, per label
    log_epoch_metrics: true

lightning_conf:
    #### args that are passed directly to the trainer
    log_every_n_steps: 250
    max_epochs: MAX_EPOCH
    strategy: ddp
    strategy_conf:
        find_unused_parameters: true

    best_model_criterion:
    -   - valid/epoch_mAP     # metric to monitor
        - max           # max or min
        - 1            # number of best models to keep
